from sklearn.decomposition import PCA, KernelPCA
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import log_loss
from sklearn.pipeline import Pipeline

import numpy as np
import scipy.io as sio
import os


def logloss(pred_train_prob, train_labels):
    train_sample_number, train_class_number = pred_train_prob.shape
    normalized_prob = np.log(np.maximum(np.minimum(pred_train_prob, 1 - 1e-15), 1e-15))
    indicator_matrix = np.zeros(pred_train_prob.shape)
    for i in range(train_sample_number):
        indicator_matrix[i, train_labels[i] - 1] = 1

    return np.sum(np.multiply(normalized_prob, indicator_matrix)) / (-train_sample_number)


def load_data(data_path):
    # load mat
    datafile = os.path.join(data_path, 'malware_feature3.mat')

    if os.path.exists(datafile) is False:
        print('Data file %s not found.' % datafile)
        return

    data_numpy = sio.loadmat(datafile)
    # get training and test data
    train_feature = data_numpy['train_feature']
    train_labels = data_numpy['train_labels']
    test_feature = data_numpy['test_feature']
    return train_feature, train_labels, test_feature


def adaboost_clf(train_feature, train_labels, test_feature):
    ab_clf = AdaBoostClassifier(n_estimators=200)
    ab_clf.fit(train_feature, train_labels.ravel())

    print(logloss(ab_clf.predict_proba(train_feature), train_labels))
    return ab_clf.predict_proba(test_feature)


def random_classifier(train_feature, train_labels, test_feature):
    rf_clf = RandomForestClassifier(n_estimators=50)
    rf_clf.fit(train_feature, train_labels.ravel())
    test_prediction_probability = rf_clf.predict_proba(test_feature)

    print(logloss(rf_clf.predict_proba(train_feature), train_labels))
    print(log_loss(train_labels, rf_clf.predict_proba(train_feature), 1e-15))

    cm = confusion_matrix(train_labels, rf_clf.predict(train_feature))
    print(cm)

    return test_prediction_probability


def gb_pipeline(train_feature, train_label, test_feature):
    kpca = PCA()
    gb_clf = GradientBoostingClassifier()
    pipe = Pipeline(steps=[
        ('dim_reduce', kpca),
        ('classify', gb_clf)])

    print('Fitting Kernel PCA on training data.')
    kpca.fit(train_feature)

    p_kpca_components = [16, 32, 64, 128, 512]
    p_gb_loss_func = ['deviance', 'log_loss']
    p_gb_n_estimators = [25, 50, 100, 150, 200]
    p_gb_max_depth = [3, 5, 7, 9]

    estimator = GridSearchCV(estimator=pipe,
                             param_grid=dict(
                                 dim_reduce__n_components=p_kpca_components,
                                 classify__loss=p_gb_loss_func,
                                 classify__n_estimators=p_gb_n_estimators,
                                 classify__max_depth=p_gb_max_depth
                             ),
                             n_jobs=24,
                             verbose=2,
                             cv=5,
                             scoring='log_loss')
    print('Fitting models using grid search')
    estimator.fit(train_feature, train_label.ravel())

    print('Best estimator:', estimator.best_estimator_)
    print('Best parameters:', estimator.best_params_)
    print('Best score:', estimator.best_score_)

    return estimator.predict_proba(test_feature)


def rf_pipeline(train_feature, train_label, test_feature):
    rf = RandomForestClassifier()
    pca = PCA()
    pipe = Pipeline(steps=[('pca', pca), ('rf', rf)])
    pca.fit(train_feature)
    n_pca_components = [16, 32, 64, 128, 256, 512]
    n_rf_estimators = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]

    estimator = GridSearchCV(estimator=pipe,
                             param_grid=dict(pca__n_components=n_pca_components,
                                             rf__n_estimators=n_rf_estimators),
                             n_jobs=8,
                             scoring='log_loss')
    estimator.fit(train_feature, train_label.ravel())

    print('Best estimator:', estimator.best_estimator_)
    print('Best parameters:', estimator.best_params_)
    print('Best score:', estimator.best_score_)

    return estimator.predict_proba(test_feature)


def gradient_boost_classifier(train_feature, train_labels, test_feature):
    gb_clf = GradientBoostingClassifier()
    gb_clf.fit(train_feature, train_labels.ravel())
    test_prediction_probability = gb_clf.predict_proba(test_feature)

    print(logloss(gb_clf.predict_proba(train_feature), train_labels))

    return test_prediction_probability


def generate_submission(submission_filename, test_sample_filename, prediction_probability):
    test_sample_number, test_prediction_number = prediction_probability.shape
    with open(submission_filename, 'w') as fid:
        fid.write('"Id","Prediction1","Prediction2","Prediction3","Prediction4","Prediction5","Prediction6","Prediction7","Prediction8","Prediction9"\n')
        for i in range(test_sample_number):
            fid.write('\"{0}\", {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8}, {9}\n'.format(test_sample_filename[i],
                                                                                      prediction_probability[i, 0],
                                                                                      prediction_probability[i, 1],
                                                                                      prediction_probability[i, 2],
                                                                                      prediction_probability[i, 3],
                                                                                      prediction_probability[i, 4],
                                                                                      prediction_probability[i, 5],
                                                                                      prediction_probability[i, 6],
                                                                                      prediction_probability[i, 7],
                                                                                      prediction_probability[i, 8]
            ))

    return

if __name__ == '__main__':
    '''
    test_sample_files = []
    test_files = os.listdir('/run/media/kewang/externalHD/Projects/kaggle/malware/test')
    for test_entry in test_files:
        if test_entry.endswith('.asm'):
            test_file_base_name, test_file_ext = os.path.splitext(test_entry)
            test_sample_files.append(test_file_base_name)

    test_sample_files.sort()
    with open('test_files.txt', 'w') as fid:
        for entry in test_sample_files:
            fid.write(entry + '\n')
    '''
    with open('test_files.txt', 'r') as fid:
        test_sample_files = fid.readlines()

        train_features, train_labels, test_features = load_data('.')
        pca_rf_pred_proba = gb_pipeline(train_features, train_labels, test_features)
        generate_submission('pca_gb_submission.csv', test_sample_files, pca_rf_pred_proba)

    '''
    pca_rf_pred_proba = gb_pipeline(train_features, train_labels, test_features)
    generate_submission('kpca_gb_submission.csv', test_sample_files, pca_rf_pred_proba)

    print('Hello random forest')
    rf_pred_proba = random_classifier(train_features, train_labels, test_features) + 0.0001
    generate_submission('rf_submission_2_hists.csv', test_sample_files, rf_pred_proba)

    print('Hello gradient boost')
    gb_pred_proba = gradient_boost_classifier(train_features, train_labels, test_features)
    generate_submission('gb_submission_2_hists.csv', test_sample_files, gb_pred_proba)

    print('Hello Adaboost')
    ab_pred_proba = adaboost_clf(train_features, train_labels, test_features)
    generate_submission('ab_submission.csv', test_sample_files, ab_pred_proba)
    '''
